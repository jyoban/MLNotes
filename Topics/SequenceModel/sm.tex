\documentclass{article}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumitem}
\setlist{nolistsep}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\title{Sequence Model}
\author{Jyotirmoy Banerjee}
\begin{document}
\maketitle

\section{Hidden Markov Model}

\begin{itemize}
\item $\mathbf{Q} = q_1,q_2, \cdots, q_N$: a set of $N$ \emph{hidden} states.
\item $\mathbf{A} = a_{11},a_{12}, \cdots, a_{n1}, \cdots, a_{nn}$: a transition probability matrix $A$, each $a_{ij}$ representing the probability of moving from state $i$ to state $j$, s.t.\ $\sum_{j=1}^{n}a_{ij} = 1 \quad  \forall i$.
\item $\mathbf{O} = o_1, o_2, \cdots, o_T$: a sequence of $T$ observations, each one drawn from a vocabulary $V = v_1, v_2, \cdots, v_V$
\item $\mathbf{B} = b_i(o_t)$: a sequence of observation likelihoods, also called emission probabilities, each expressing the probability of an observation $o_t$ being generated from a state $i$.
\item $\mathbf{q_0,q_F}$: a special start state and end (final) state which are not associated with observations, together with transition probabilities $a_{01},a_{02}, \cdots, a_{0n}$ out of the start state and $a_{n1},a_{n2}, \cdots, a_{nn}$ out of the end state.
\end{itemize}

A first-order hidden Markov model instantiates two simplifying assumptions.
First, as with a first-order Markov chain, the probability of a particular state depends
only on the previous state:
\[\text{Markov Assumption}:  P(q_i | q_1,\cdots q_{i-1}) = P(q_i | q_{i-1}) \]

Second, the probability of an output observation $o_i$ depends only on the state that
produced the observation $q_i$ and not on any other states or any other observations:
\[\text{Output Independence}: P(o_i|q_1,\cdots,q_i,\cdots,q_T,o_1,\cdots,o_i,\cdots,o_T) = P(o_i | q_i)\]

The joint probability of hidden states and observation is:

\[ P(O, Q) = P(O|Q) P(Q) = \prod_{i=1}^{n} P(o_i | q_i) \prod_{i-1}^{n} P(q_i | q_{i-1}) \]


For an HMM with $N$ hidden states and an observation sequence of $T$ observations, there are $N^T$ possible hidden sequences. For real tasks, where $N$ and $T$ are both large, $N^T$ is a very large number, and so we cannot compute the total observation likelihood by computing a separate observation likelihood for each hidden state sequence and then summing them up. Instead of using such an extremely exponential algorithm, we use an efficient $(O(N^2 T))$ algorithm called the forward algorithm. The forward algorithm is a kind of dynamic programming algorithm, i.e., an algorithm that uses a table to store intermediate values as it builds up the probability of the observation sequence.

A limitations of HMM is it acquires the joint distribution $P(Y, X)$ of the state and the observed sequence, while in the estimation issue, we need a conditional probability $P(Y|X)$.

\section{Generative vs. Discriminative}
Both generative models and discriminative models describe distributions over $(y,x)$, but they work in different directions. A generative model, such as the naive Bayes classifier and the HMM, is a family of joint distributions that factorizes as $p(y,x) = p(y)p(x|y)$, that is, it describes how to sample, or ``generate'', values for features given the label. A discriminative model, such as the logistic regression model, is a family of conditional distributions $p(y|x)$, that is, the classification rule is modeled directly. 

%https://www.pure.ed.ac.uk/ws/portalfiles/portal/10482724/crftut_fnt.pdf

The main conceptual difference between discriminative and generative models is that a conditional distribution $p(y|x)$ does not include a model of $p(x)$, which is not needed for classification anyway. The difficulty in modeling $p(x)$ is that it often contains many highly dependent features that are difficult to model. The principal advantage of discriminative modeling is that it is better suited to including rich, overlapping features. 

By modeling the conditional distribution directly, we can remain agnostic about the form of $p(x)$. Discriminative models, such as CRFs, make conditional independence assumptions among $y$, and assumptions about how the y can depend on x, but do not make conditonal independence assumptions among x.


Suppose we have a generative model $p_g$ with parameters $\theta$. By definition, this takes
the form

\[p_g(y, x; \theta) = p_g(y; \theta)p_g(x|y; \theta). \]

Now, compare this generative model to a discriminative model over
the same family of joint distributions. To do this, we define a prior
$p(x)$ over inputs, such that $p(x)$ could have arisen from $p_g$ with some
parameter setting. That is, $p(x) = p_c(x; \theta') = \sum_y p_g(y, x|\theta')$. 
We combine this with a conditional distribution $p_c(y|x; \theta)$ that could also have
arisen from $p_g$, that is, $p_c(y|x; \theta) = p_g(y, x; \theta)/p_g(x; \theta)$. Then the resulting distribution is

\[p_c(y, x) = p_c(x; \theta')p_c(y|x; \theta).\]

By comparing above equations, it can be seen that the conditional
approach has more freedom to fit the data, because it does not require that $\theta = \theta'$. 



\section{Linear-chain CRFs}
To motivate our introduction of linear-chain CRFs, we begin by considering the conditional distribution $p(y|x)$ that follows from the joint distribution $p(y,x)$ of an HMM. The key point is that this conditional distribution is in fact a CRF with a particular choice of feature functions.



\end{document}

