\documentclass{article}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumitem}
\setlist{nolistsep}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\title{Mixture Model}
\author{Jyotirmoy Banerjee}
\begin{document}
\maketitle


\section{Introduction}
We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution). Then we proceed to estimate parameters of this distribution, like the mean and variance, using maximum likelihood estimation.

However, in many cases, assuming each sample comes from the same unimodal distribution is too restrictive and may not make intuitive sense. Often the data we are trying to model are more complex. For example, they might be multimodal – containing multiple regions with high probability mass. In this note, we describe mixture models which provide a principled approach to modeling such complex data.


\textbf{Definition}:
Assume we observe $X_1,\cdots,X_n$ and that each $X_i$ is sampled from one of $K$ mixture components. For example, the mixture components could be {male,female}. Associated with each random variable $X_i$ is a label $Z_i \in {1,\cdots,K}$ which indicates which component $X_i$ came from. In our height example, $Z_i$ would be either 1 or 2 depending on whether $X_i$ was a male or female height. Often times we don’t observe $Z_i$ (e.g. we might just obtain a list of heights with no gender information), so the $Z_i$'s are sometimes called latent variables.

From the law of total probability, we know that the marginal probability of $X_i$ is:
\[ P(X_i) = \sum_{k=1}^{K} P(X_i | Z_i) P(Z_i) = \sum_{k=1}^{K} P(X_i | Z_i) \pi_k \]

Here, the $\pi_k$ are called mixture proportions or mixture weights and they represent the probability that $X_i$ belongs to the $k$-th mixture component. The mixture proportions are nonnegative and they sum to one, $\sum_{k=1}^{K} \pi_k = 1$. We call $P(X_i|Z_i=k)$ the mixture component, and it represents the distribution of $X_i$ assuming it came from component $k$.

If we observe independent samples $X_1,\cdots,X_n$ from this mixture, with mixture proportion vector $\pi=(\pi_1,\pi_2,\cdots,\pi_K)$, then the likelihood function is:

\[
L(\pi) = \prod_{i=1}^{n} P(X_i | \pi) = \prod_{i=1}^{n} \sum_{k=1}^{K} P(X_i | Z_i) \pi_k
\]

In the Gaussian mixture model setting the $k$-th component is $\mathcal{N}(\mu_k,\sigma_k)$ and the mixture proportions are $\pi_k$.

\end{document}

