\documentclass{article}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumitem}
\setlist{nolistsep}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\title{Mixture Model}
\author{Jyotirmoy Banerjee}
\begin{document}
\maketitle


\section{Introduction}
We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution). Then we proceed to estimate parameters of this distribution, like the mean and variance, using maximum likelihood estimation.

However, in many cases, assuming each sample comes from the same unimodal distribution is too restrictive and may not make intuitive sense. Often the data we are trying to model are more complex. For example, they might be multimodal – containing multiple regions with high probability mass. In this note, we describe mixture models which provide a principled approach to modeling such complex data.


\textbf{Definition}:
Assume we observe $X_1,\cdots,X_n$ and that each $X_i$ is sampled from one of $K$ mixture components. For example, the mixture components could be {male,female}. Associated with each random variable $X_i$ is a label $Z_i \in {1,\cdots,K}$ which indicates which component $X_i$ came from. In our height example, $Z_i$ would be either 1 or 2 depending on whether $X_i$ was a male or female height. Often times we don’t observe $Z_i$ (e.g. we might just obtain a list of heights with no gender information), so the $Z_i$'s are sometimes called latent variables.

From the law of total probability, we know that the marginal probability of $X_i$ is:
\[ P(X_i) = \sum_{k=1}^{K} P(X_i | Z_i) P(Z_i) = \sum_{k=1}^{K} P(X_i | Z_i) \pi_k \]

Here, the $\pi_k$ are called mixture proportions or mixture weights and they represent the probability that $X_i$ belongs to the $k$-th mixture component. The mixture proportions are nonnegative and they sum to one, $\sum_{k=1}^{K} \pi_k = 1$. We call $P(X_i|Z_i=k)$ the mixture component, and it represents the distribution of $X_i$ assuming it came from component $k$.

If we observe independent samples $X_1,\cdots,X_n$ from this mixture, with mixture proportion vector $\pi=(\pi_1,\pi_2,\cdots,\pi_K)$, then the likelihood function is:

\[
L(\pi) = \prod_{i=1}^{n} P(X_i | \pi) = \prod_{i=1}^{n} \sum_{k=1}^{K} P(X_i | Z_i) \pi_k
\]

In the Gaussian mixture model setting the $k$-th component is $\mathcal{N}(\mu_k,\sigma_k)$ and the mixture proportions are $\pi_k$.

\section{Expectation Maximization}:
The EM algorithm attempts to find maximum likelihood estimates for models with latent variables. In this section, we describe a more abstract view of EM which can be extended to other latent variable models.

Let $X$ be the entire set of observed variables and $Z$ the entire set of latent variables. The log-likelihood is therefore:

\[ 
\log {(P(X|\Theta))} = \log \left( {\sum_Z P(X, Z |\Theta)} \right)
\]

The existence of the sum inside the logarithm prevents us from applying the log to the densities which results in a complicated expression for the MLE. Now suppose that we observed both $X$ and $Z$. We call $\{X,Z\}$ the complete data set, and we say $X$ is incomplete. As we noted previously, if we knew $Z$, the maximization would be easy.

We typically don't know $Z$, but the information we do have about $Z$ is contained in the posterior $P(Z|X,\theta)$. Since we don't know the complete log-likelihood, we consider its expectation under the posterior distribution of the latent variables. This corresponds to the E-step above. In the M-step, we maximize this expectation to find a new estimate for the parameters.

In the E-step, we use the current value of the parameters $\theta^0$ to find the posterior distribution of the latent variables given by $P(Z|X,\theta^0)$. We then use this to find the expectation of the complete data log-likelihood, with respect to this posterior, evaluated at an arbitrary $\theta$. This expectation is denoted $Q(\theta,\theta^0)$ and it equals:

\[ 
Q(\theta, \theta^0) = E_{Z | X, \theta^0} [log(P(X,Z|\theta))] = \sum_Z P(Z | X, \theta^0) log(P(X,Z|\theta))
\]

In the M-step, we determine the new parameter $\hat{\theta}$ by maximizing Q:
\[
\hat{\theta} =  \argmax_{\theta} Q(\theta, \theta^0) 
\]

\end{document}

