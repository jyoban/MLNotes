\documentclass{article}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumitem}
\setlist{nolistsep}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\title{Mixture Model}
\author{Jyotirmoy Banerjee}
\begin{document}
\maketitle


\section{Introduction}
We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution). Then we proceed to estimate parameters of this distribution, like the mean and variance, using maximum likelihood estimation.

However, in many cases, assuming each sample comes from the same unimodal distribution is too restrictive and may not make intuitive sense. Often the data we are trying to model are more complex. For example, they might be multimodal – containing multiple regions with high probability mass. In this note, we describe mixture models which provide a principled approach to modeling such complex data.


\textbf{Definition}:
Assume we observe $X_1,\cdots,X_n$ and that each $X_i$ is sampled from one of $K$ mixture components. For example, the mixture components could be {male,female}. Associated with each random variable $X_i$ is a label $Z_i \in {1,\cdots,K}$ which indicates which component $X_i$ came from. In our height example, $Z_i$ would be either 1 or 2 depending on whether $X_i$ was a male or female height. Often times we don’t observe $Z_i$ (e.g. we might just obtain a list of heights with no gender information), so the $Z_i$'s are sometimes called latent variables.

From the law of total probability, we know that the marginal probability of $X_i$ is:
\[ P(X_i) = \sum_{k=1}^{K} P(X_i | Z_i) P(Z_i) = \sum_{k=1}^{K} P(X_i | Z_i) \pi_k \]

Here, the $\pi_k$ are called mixture proportions or mixture weights and they represent the probability that $X_i$ belongs to the $k$-th mixture component. The mixture proportions are nonnegative and they sum to one, $\sum_{k=1}^{K} \pi_k = 1$. We call $P(X_i|Z_i=k)$ the mixture component, and it represents the distribution of $X_i$ assuming it came from component $k$.

If we observe independent samples $X_1,\cdots,X_n$ from this mixture, with mixture proportion vector $\pi=(\pi_1,\pi_2,\cdots,\pi_K)$, then the likelihood function is:

\[
L(\pi) = \prod_{i=1}^{n} P(X_i | \pi) = \prod_{i=1}^{n} \sum_{k=1}^{K} P(X_i | Z_i) \pi_k
\]

In the Gaussian mixture model setting the $k$-th component is $\mathcal{N}(\mu_k,\sigma_k)$ and the mixture proportions are $\pi_k$.

\section{MLE for Gaussian Mixture}

Now we attempt the same strategy for deriving the MLE of the Gaussian mixture model. Our unknown parameters are $\theta={\mu_1,\cdots,\mu_K,\sigma_1,\cdots,\sigma_K,\pi_1,\cdots,\pi_K}$, and so from the first section of this note, our likelihood is:

\[
L(\theta | X_1, \cdots ,X_n) =  \prod_{i=1}^{n} \sum_{k=1}^{K}  \pi_k \mathcal{N} (x_i ; \mu_k, {\sigma_k}^2)
\]

So our log-likelihood is:

\[
l(\theta) = \sum_{i=1}^{n} \log \left( \sum_{k=1}^{K}  \pi_k \mathcal{N} (x_i ; \mu_k, {\sigma_k}^2) \right)
\]

\section{Expectation Maximization - Intuition}
If we were to differentiate $l(\theta)$ with respect to $\mu_k$ and set the expression equal to zero, we would get:

\[
\sum_{i=1}^{n} \frac{1}{\sum_{k=1}^{K}  \pi_k \mathcal{N} (x_i ; \mu_k, {\sigma_k}^2)} {\pi_k \mathcal{N} (x_i ; \mu_k, {\sigma_k}^2}) \frac{\left(x_i - \mu_k\right)}{{\sigma_k}^2}
\]


Step 1:\\
\[
\gamma_{Z_i} (k) =  \frac{\pi_k \mathcal{N} (\mu_k, {\sigma_k}^2)}{\sum_{k=1}^{K}  \pi_k \mathcal{N} (\mu_k, {\sigma_k}^2)} {}
\]

Step 2:\\
\[
\sum_{i=1}^{n} \gamma_{Z_i} (k) \frac{\left(x_i - \mu_k\right)}{{\sigma_k}^2} = 0
\]
\[
\hat{\mu_k} = \frac{\sum_{i=1}^{n} \gamma_{Z_i} (k) x_i}{\sum_{i=1}^{n} \gamma_{Z_i} (k)} = \frac{1}{N_k} \sum_{i=1}^{n} \gamma_{Z_i} (k) x_i
\]

Where we set $N_k=\sum_{i=1}^{n}\gamma_{z_i}(k)$. We can think of $N_k$ as the effective number of points assigned to component $k$.
Similarly, if we apply a similar method to finding $\hat{\sigma}^{2}_{k}$ and $\hat{\pi}_{k}$, we find that:

\[
\hat{\sigma}^{2}_{k} = \frac{1}{N_k} \sum_{i=1}^{n} \gamma_{Z_i} (k) (x_i - \mu_k)^2
\]
\[
\hat{\pi}_{k} = \frac{N_k}{n}
\]

%\section{Expectation Maximization}
%The EM algorithm attempts to find maximum likelihood estimates for models with latent variables. In this section, we describe a more abstract view of EM which can be extended to other latent variable models.

%Let $X$ be the entire set of observed variables and $Z$ the entire set of latent variables. The log-likelihood is therefore:

%\[ 
%\log {(P(X|\Theta))} = \log \left( {\sum_Z P(X, Z |\Theta)} \right)
%\]

%The existence of the sum inside the logarithm prevents us from applying the log to the densities which results in a complicated expression for the MLE. Now suppose that we observed both $X$ and $Z$. We call $\{X,Z\}$ the complete data set, and we say $X$ is incomplete. As we noted previously, if we knew $Z$, the maximization would be easy.

%We typically don't know $Z$, but the information we do have about $Z$ is contained in the posterior $P(Z|X,\theta)$. Since we don't know the complete log-likelihood, we consider its expectation under the posterior distribution of the latent variables. This corresponds to the E-step above. In the M-step, we maximize this expectation to find a new estimate for the parameters.

%In the E-step, we use the current value of the parameters $\theta^0$ to find the posterior distribution of the latent variables given by $P(Z|X,\theta^0)$. We then use this to find the expectation of the complete data log-likelihood, with respect to this posterior, evaluated at an arbitrary $\theta$. This expectation is denoted $Q(\theta,\theta^0)$ and it equals:

%\[ 
%Q(\theta, \theta^0) = E_{Z | X, \theta^0} [log(P(X,Z|\theta))] = \sum_Z P(Z | X, \theta^0) \log(P(X,Z|\theta))
%\]

%In the M-step, we determine the new parameter $\hat{\theta}$ by maximizing Q:
%\[
%\hat{\theta} =  \argmax_{\theta} Q(\theta, \theta^0) 
%\]

% https://stephens999.github.io/fiveMinuteStats/intro_to_em.html


\section{Expectation Maximization}

Statistical inference involves finding the right model and parameters that represent the distribution of observations well. Let $\mathbf{x}$ be the observations and $\theta$ be the unknown parameters of a ML model. In maximum likelihood estimation, we try to find the $\theta_\mathrm{ML}$
that maximizes the probability of the observations using the ML model with the parameters:
\begin{equation}
\hat{\theta}_\mathrm{ML} = \argmax_\theta \, p(\mathbf{x}, \theta)
\end{equation}
Typically, the problem requires few assumptions to solve the above optimization efficiently. One trick is to introduce latent variables $\mathbf{z}$ that break down the problem into smaller subproblems. For instance, in the Gaussian Mixture Model, we can introduce the cluster membership assignment as random variables $z_i$ for each datum $x_i$, which greatly simplifies the model
$(p(x_i|z_i=k) \sim \mathcal{N}(\mu_k,\sigma_k))$.
\begin{equation}
p(\mathbf{x};\theta)=\int p(\mathbf{x},\mathbf{z};\theta) d{z}
\end{equation}
However, the above integration is, in many cases, intractable and can be either approximated using stochastic sampling (Monte Carlo methods) or we can simply bypass the computation using few assumptions. The second method is called variational inference, coined after the calculus of variations, which we will go over.

\subsection{Evidence Lower Bound (ELBO)}

Let $q(z)$ be a probability distribution on z. Then,
\begin{align}
\ln p(x;\theta) &= \int q(z) \ln p(x; \theta) dz \nonumber \\
&= \int q(z) \ln \left( \frac{p(x; \theta) p(z|x; \theta)}{p(z|x; \theta)} \right) dz \nonumber \\
&= \int q(z) \ln \left( \frac{p(x,z; \theta)}{p(z|x; \theta)} \right) dz \nonumber \\
&= \int q(z) \ln \left( \frac{p(x,z; \theta)q(z)}{p(z|x; \theta)q(z)} \right) dz \nonumber \\
&= \int q(z) \ln \left( \frac{p(x,z; \theta)}{q(z)} \right) dz - \int q(z) \ln \left( \frac{p(z|x; \theta)}{q(z)} \right) dz \nonumber \\
&= F(q,\theta) + KL(q||p) \nonumber 
\end{align}

where $F(q,\theta)$ known as the evidence lower bound or ELBO, or the negative of the variational free energy. $KL$ is the Kullback-Leibler divergence. Since the KL-divergence is non-negative,
\[ \ln p(x;\theta) \geq F(q,\theta) \]

The ELBO provides a lower bound for the marginal likelihood. Instead of maximizing the marginal likelihood directly, the Expectation Maximization (EM) and variational inference maximize the variational lower bound.

Let's assume that we can find $p(z|x; \theta_\text{OLD})$ analytically (for the Gaussian Mixture Model, this is just a softmax). Then, we can simply substitute $q(z)=p(z|x;\theta_\text{OLD})$. The ELBO becomes

\begin{align}
F(q,\theta) &= \int q(z) \ln \left( \frac{p(x,z; \theta)}{q(z)} \right) dz \nonumber \\
&= \int q(z) \ln p(x,z; \theta)  dz - \int q(z) \ln q(z) dz  \nonumber \\
&= \int p(z|x;\theta_\text{OLD}) \ln p(x,z; \theta)  dz - \int p(z|x;\theta_\text{OLD}) \ln p(z|x;\theta_\text{OLD}) dz  \nonumber  \\
&= Q(\theta, \theta_\text{OLD}) + H(q)
\end{align}
The second term is entropy. It is a constant with respect to $\theta$ and we do not take the term into account while maximizing the ELBO.

The EM algorithm can be succinctly summarized as $\argmax_{\theta} Q(\theta, \theta_\text{OLD})$
\begin{itemize}
\item E-step: compute $p(z|x;\theta_\text{OLD})$
\item M-step: evaluate $\argmax_{\theta} \int p(z|x;\theta_\text{OLD}) \ln p(x,z; \theta)  dz$
\end{itemize}

For example, the EM for the Gaussian Mixture Model consists of an expectation step where you compute the soft assignment of each datum to K clusters, and a maximization step which computes the parameters of each cluster using the assignment. However, for complex models, we cannot use the EM algorithm.

\end{document}

