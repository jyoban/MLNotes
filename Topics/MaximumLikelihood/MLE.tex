\documentclass{article}

\usepackage{hyperref}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\dd}[1]{\mathrm{d}#1}


\title{Maximum likelihood estimation}
\author{Jyotirmoy Banerjee}
\begin{document}
\maketitle

\section{Introduction}
\textbf{Probability mass function:} 
A discrete random variable is a random variable whose range is finite or countably infinite. The probability mass function of a discrete random variable $X$ is 
\[ f_X(x) = P\{X=x\}\]
The mass function has two basic properties:
\begin{itemize}
\item $f_X(x) \geq 0$ for all $x$ in the state space
\item $\sum_x f_X(x) = 1$
\end{itemize}

\noindent
\textbf{Probability density function:} Let $X$ be  a  random  variable  whose  cumulative distribution function $F_X$ has a derivative. The  function $f_X$ satisfying
\[ F_X(x) = \int _{-\infty }^{x}f_{X}(t)\,dt \]
is called the probability density function and $X$ is called a continuous random variable.
By the fundamental theorem of calculus, ${F}'_X(x) = f_X(x)$. We can compute compute probabilities using
\[P\{a < X \leq b\} = F_X(b) - F_X(a) = \int _{a}^{b}f_{X}(t)\,dt\] 

\noindent
\textbf{Independence:} $X$ and $Y$ are independent if:
\begin{align*}
P(X,Y) &= P(X)P(Y) \\
\text{Joint} &= \text{Marginal} \times \text{Marginal}
\end{align*}

\noindent
\textbf{Conditional probability:} Probability of $X$ given that $Y$ happened:
\begin{align*}
P(X \vert Y) &= \frac{P(X,Y)}{P(Y)} \\
\text{Conditional} &= \frac{\text{Joint}}{\text{Marginal}}
\end{align*}

\noindent
\textbf{Chain rule:} 
\begin{align*}
P(X,Y) &= P(X \vert Y)P(Y) \\
P(X, Y, Z) &= P(X \vert Y, Z) P(Y \vert Z) P(Z) \\
P(X_1,X_2,\dots,X_N) &= \prod_{i=1}^{N}P(X_i \vert X_1,X_2,\dots,X_{i-1})
\end{align*}

\noindent
\textbf{Sum rule:} Marginalization
\begin{align*}
p(X) &= \int _{-\infty }^{\infty} p(X,Y) \, dY
\end{align*} 

\noindent
\textbf{Bayes theorem}
\begin{itemize}
\item $\theta$ - Parameters
\item $X$ - Observations
\end{itemize}
\begin{align*} 
P(\theta \vert X) &= \frac{P(\theta, X)}{P(X)} \\
&= \frac{P(X \vert \theta)P(\theta)}{P(X)} \\
\text{Posterior} &= \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}} \\
& \propto \text{Likelihood} \times \text{Prior}
\end{align*}

\section{Likelihood}
Given a statistical probability mass function or density, say $f(x,\theta)$, where $\theta$ is an unknown parameter, the \emph{likelihood} is $f$ viewed as a function of $\theta$ for a fixed, observed value of $x$ of the random variable $X$. The probability density function is the continuous analogue of probability mass function.

Given the outcome $x$ of the random variable $X$, the likelihood function, which is a function of $\theta$, is given as:
\begin{equation*}
\mathcal{L}(\theta|x) = f_{\theta}(x) = P_{\theta}(X = x) = P(X = x \mid \theta) 
\end{equation*} 

For example -
\begin{enumerate} \addtolength{\itemsep}{-0.5\baselineskip}
\item Suppose we flip a coin with success probability of $\theta$
\item Recall that the mass function for $x$
\[ f(x, \theta) = {\theta}^x{(1-\theta)^{1-x}} \quad \mbox{for} \quad \theta \in [0, 1]\] 
where $x$ is either 0 (tails) or 1 (heads)
\item Suppose that the result is a head. The likelihood is 
\[ \mathcal{L}(\theta|1) = {\theta}^1{(1-\theta)^{1-1}} = \theta \quad \mbox{for} \quad \theta \in [0, 1]\]
\item Therefore, $\mathcal{L}(0.5|1)/\mathcal{L}(0.25|1) = 2$
\item There is twice as much evidence supporting the hypothesis that $\theta = 0.5$ to the hypothesis that $\theta = 0.25$
\end{enumerate}

\section{Maximum likelihood and Maximum a posteriori}
Maximum a posteriori (MAP) and maximum likelihood estimation (MLE) are stated as follows
\begin{align*}
\theta_{\mathcal{MAP}} &= \argmax_{\theta} P(\theta|x) \nonumber \\
									&= \argmax_{\theta} P(x|\theta) P(\theta)  \nonumber \\
\theta_{\mathcal{MLE}} &= \argmax_{\theta} P(x|\theta) \nonumber
\end{align*}
where $x$ is the input data and $\theta$ is the output parameter. $\mathcal{L}_\theta = P(x|\theta)$.


\section{Maximum likelihood}

\begin{itemize}
\item Let's suppose we have observed 10 data points from some process. For example, each data point could represent the length of time in seconds that it takes a student to answer a specific exam question.
\item We assume that they have been generated from a process that is adequately described by a Gaussian distribution. How do we calculate the maximum likelihood estimates of the parameter values of the Gaussian distribution $\mu$ and $\sigma$?
\item Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.
\item What we want to calculate is the total probability of observing all of the data, i.e.\ the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we will make our first assumption. The assumption is that each data point is generated independently of the others. This assumption makes the maths much easier. 
\item If the events (i.e.\ the process that generates the data) are independent, then the total probability of observing all of data is the product of observing each data point individually (i.e. the product of the marginal probabilities).
\end{itemize}



The probability density of observing a single data point x, that is generated from a Gaussian distribution is given by:
\[
P(x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]
The semi colon used in the notation $P(x; \mu, \sigma)$ is there to emphasise that the symbols that appear after it are parameters of the probability distribution. So it should not be confused with a conditional probability (which is typically represented with a vertical line e.g.\ $P(A| B)$).


The total (joint) probability density of observing n data points is given by:
\[
P(x_1,x_2,\cdots,x_n;\mu,\sigma) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \exp \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right)
\]
We just have to figure out the values of $\mu$ and $\sigma$ that results in giving the maximum value of the above expression. This expression can be differentiated to find the maximum.

The above expression for the total probability is difficult to differentiate, so it is almost always simplified by taking the natural logarithm of the expression. This is absolutely fine because the natural logarithm is a monotonically increasing function. This means that if the value on the x-axis increases, the value on the y-axis also increases. This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.

\section{Bayes or Maximum likelihood classifier}
The maximum likelihood classifier is one of the most popular methods of classification, in which x with the maximum likelihood is classified into the corresponding class. The likelihood $L_k$ is defined as the posterior probability of a pixel belonging to class k.

\[
L_k = P(k|x) = \frac{P(x|k)P(k)}{P(x)} = \frac{P(x|k)P(k)}{\sum P(x|i)P(i)}
\]
where $P(k)$ is the probability of class k.

Usually $P(k)$ are assumed to be equal to each other and $P(i)*P(x|i)$ is also common to all classes. Therefore $L_k$ depends on $P(x|k)$ or the probability density function.

For mathematical reasons, a multivariate normal distribution is applied as the probability density function. In the case of normal distributions, the likelihood can be expressed as follows.

\[
L_k = \frac{1}{\sigma\sqrt{2\pi}} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]

\section{Na\"{\i}ve Bayes}
The Na\"{\i}ve Bayes classifier is an approximation to the Bayes classifier, in which we assume that the features are conditionally independent given the class instead of modelling their full conditional distribution given the class.
\begin{align*} 
P(k|x_1,x_2,\cdots,x_n) &\propto  P(x_1,x_2,x_3,\cdots,x_n,k) \\ 
&=  P(x_1|x_2,x_3,\cdots,x_n,k)P(x_2,x_3,\cdots,x_n,k) \\ 
&= P(x_1|x_2,x_3,\cdots,x_n,k)P(x_2|x_3,\cdots,x_n,k)P(x_3,\cdots,x_n,k) \\
&=  P(x_1|x_2,x_3,\cdots,x_n,k)P(x_2|x_3,\cdots,x_n,k) \cdots P(x_n|k)P(k) \\ 
&=  P(k)P(x_1|k)P(x_2|k) \cdots P(x_n|k) \\
&=  P(k)\prod_{i=1}^{n} P(x_i|k)
\end{align*}

\begin{align*} 
P(k|x_1,x_2,\cdots,x_n) &= P(k)\prod_{i=1}^{n} P(x_i|k)
\end{align*}

\subsection{Multinomial Na\"{\i}ve Bayes}
With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial  $(p_1,\dots,p_n)$ where $p_i$ is the probability that event i occurs. A feature vector $\mathbf {x}=(x_{1},\dots ,x_{n})$ is then a histogram, with $x_{i}$ counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram $\mathbf{x}$ is given by:

\[
p(\mathbf {x} \mid C_{k}) = {\frac {(\sum _{i}x_{i})!}{\prod _{i}x_{i}!}} \prod _{i}{p_{ki}}^{x_{i}}
\]

If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing na\"{\i}ve Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.

In the context of document classification and possible ways to alleviate those problems is by including the use of tf–idf weights instead of raw term frequencies and document length normalization, to produce a na\"{\i}ve Bayes classifier that is competitive with support vector machines.

\section{Maximum likelihood regression}
The distribution of $X$ is arbitrary. If $X=x$, then $Y=\beta_0+\beta_1 x+\epsilon$, for some parameters $\beta_0$ and $\beta_1$, and some random noise variable $\epsilon$. The noise is independent of $X$.

Because of these stronger assumptions, the model tells us the conditional pdf of $Y$ for each $x$, $p(y|X=x;\beta_0,\beta_1,\sigma)$. Given any data set $(x_1,y_1),(x_2,y_2),\cdots (x_n,y_n)$, we can now write down the probability density, under the model, of seeing that data:
\[
\prod_{i=1}^n p(y_i|x_i;\beta_0,\beta_1,\sigma) = \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}}  \exp \left( -\frac{(y_i-(\beta_0+\beta_1 x_i+\epsilon))^2}{2\sigma^2} \right)
\]
In multiplying together the probabilities like this, we are using the independence of the $Y_i$. This is the likelihood function. In the method of maximum likelihood, we p[ck the parameter values which  maximize  the  likelihood,  or,  equivalently,  maximize  the  log-likelihood.

\section{Logistic regression using maximum likelihood formulation}
Some definitions:
Odds = $\frac{p}{1-p}$, logit = log odds

Assume that $Pr(Y=1|X=x)=p(x;θ)$, for some function parametrized by $\theta$. Further assume that observations are independent of each other. Then the (conditional) likelihood function is:
\[
\prod_{i=1}^{n} Pr(Y=y_i|X=x_i) = \prod_{i=1}^{n} p(x_i;\theta)^y_i (1-p(x_i;\theta)^{1-y_i}) 
\]

\end{document}

